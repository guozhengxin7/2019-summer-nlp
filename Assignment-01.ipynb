{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 请回答以下问题\n",
    "\n",
    "回答以下问题，并将问题发送至 mqgao@kaikeba.com中：\n",
    "```\n",
    "    2.1. what do you want to acquire in this course？\n",
    "    2.2. what problems do you want to solve？\n",
    "    2.3. what’s the advantages you have to finish you goal?\n",
    "    2.4. what’s the disadvantages you need to overcome to finish you goal?\n",
    "    2.5. How will you plan to study in this course period?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 如何提交\n",
    "代码 + 此 jupyter 相关，提交至自己的 github 中(**所以请务必把GitHub按照班主任要求录入在Trello中**)；\n",
    "第2问，请提交至mqgao@kaikeba.com邮箱。\n",
    "#### 4. 作业截止时间\n",
    "此次作业截止时间为 2019.7.6日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 语音识别、自动驾驶、人脸识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 用Github交作业？😂 因为jupyter方便演示以及可视化，而Pycharm作为IDE在实际开发中可以提高效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:从已有的语料中，可以统计得到相邻词之间出现的概率，从而可以判断给定的两个词出现的概率以及多个词组成的语句的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:根据语法规则生成的语料的合理性/输入过程中自动纠错或智能提示?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:因为要做语义理解的话，很难做到完全根据语法规则把一句话规范的分解成语法规则的各个部分，可能这句话本身就是有语法错误的。使用概率模型可以有更好的容错性。难点在于前期需要处理大量的语料，包括清洗、分词、统计词之间的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:语言模型是一个关于多个词的概率分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:这个问题是同3吗？根据语法规则生成的语料的合理性/输入过程中自动纠错或智能提示?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:一元文法指考虑单个词的出现概率，然后以所有词同时出现的概率作为句子的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:一元文法只考虑单个词出现的概率，优点就是计算简单，缺点是这样算出来效果并不好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:二元文法考虑相邻两个词出现的概率，计算所有两两词同时出现的概率作为句子的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammar(grammar_str, split='=', line_split='\\n'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split(line_split):\n",
    "        if not line.strip(): continue\n",
    "        exp, stmt = line.split(split)\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "choice = random.choice\n",
    "\n",
    "def generate(gram, target, sep=''):\n",
    "    if target == \"space\": return \" \"\n",
    "    elif target not in gram: return target  # means target is a terminal expression\n",
    "\n",
    "    expaned = [generate(gram, t, sep) for t in choice(gram[target])]\n",
    "    return sep.join([e if e != '/n' else '\\n' for e in expaned if e != 'null'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汇编语句 包含 mov add sub 三条指令，支持直接寻址/间接寻址/立即数\n",
    "\n",
    "asm_grammar_str = '''\n",
    "asm_stmt = move_stmt | add_stmt | sub_stmt\n",
    "move_stmt = move space addr_exp , value_exp\n",
    "add_stmt = add space addr_exp , value_exp\n",
    "sub_stmt = sub space addr_exp , value_exp\n",
    "value_exp = const | addr_exp\n",
    "const = num num*\n",
    "num* = null | num num*\n",
    "num = 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\n",
    "addr_exp = direct_addr | indirect_addr\n",
    "direct_addr = reg\n",
    "reg = eax | ebx | ecx | edx\n",
    "indirect_addr = [ reg ]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "asm_grammar = create_grammar(asm_grammar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'add [ebx],7354'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(asm_grammar, \"asm_stmt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打招呼，说自己的名字和爱好\n",
    "hello_grammar_str = '''\n",
    "hello_stmt = 打招呼 , 说名字 , 说爱好\n",
    "打招呼 = 你们好 | 大家好 | 早上好 | Hello | Hi\n",
    "说名字 = 主语 名字动词 名字\n",
    "主语 = 我 | 俺 | 朕  | 我们 | 你们 | 他们\n",
    "名字动词 = 是 | 叫  \n",
    "名字 = 小明 | 小红 | 周杰伦 | Tony | Alice\n",
    "说爱好 = 主语 爱好动词 爱好\n",
    "爱好动词 = 喜欢 | 热爱\n",
    "爱好 = 动词 名词\n",
    "动词 = 看 | 吃 | 穿 | 玩 | 唱 | 打 | 爬 | 游\n",
    "名词 = 电影 | 篮球 | 乒乓球 | 歌 | 衣服 | 美食 | 山 | 水\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_grammar = create_grammar(hello_grammar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你们好,我们是Alice,他们热爱吃篮球'"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(gram=hello_grammar, target=\"hello_stmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(gram_str, target, n=20):\n",
    "    gram = create_grammar(gram_str)\n",
    "    return [generate(gram, target) for i in range(0, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi,我们是Tony,俺热爱玩山',\n",
       " '大家好,朕是周杰伦,朕热爱吃水',\n",
       " '你们好,他们叫Tony,朕喜欢玩电影',\n",
       " '大家好,他们是Alice,你们喜欢游篮球',\n",
       " '早上好,俺叫Alice,朕热爱打乒乓球',\n",
       " '大家好,我是Alice,我们热爱玩篮球',\n",
       " '大家好,朕叫小明,俺热爱穿篮球',\n",
       " '早上好,你们是小明,俺热爱游乒乓球',\n",
       " 'Hi,俺叫Tony,我们热爱穿水',\n",
       " '早上好,我叫Tony,朕喜欢爬水',\n",
       " '早上好,我叫Alice,我们喜欢打电影',\n",
       " '大家好,俺叫小明,朕热爱打衣服',\n",
       " '你们好,你们是小红,朕喜欢唱美食',\n",
       " '早上好,俺是Tony,我们热爱唱歌',\n",
       " 'Hello,俺叫Alice,我们热爱游乒乓球',\n",
       " '你们好,他们是周杰伦,他们喜欢唱山',\n",
       " '早上好,俺叫Alice,我热爱打乒乓球',\n",
       " '早上好,俺叫小红,俺喜欢玩歌',\n",
       " 'Hi,我是Alice,我热爱吃山',\n",
       " '早上好,他们是周杰伦,朕喜欢玩乒乓球']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n(hello_grammar_str, target=\"hello_stmt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "import fileinput\n",
    "from collections import Counter\n",
    "# jieba.enable_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_frequency(stator, sentence):\n",
    "    \"\"\"计算词频\"\"\"\n",
    "    # 简单的只保留 w\n",
    "    ss = re.findall('\\w+', sentence)\n",
    "    for s in ss:\n",
    "        # 一元文法\n",
    "        tokens = jieba.lcut(s)\n",
    "        stator['uni_gram_token_cnt'] += len(tokens)\n",
    "        stator['uni_gram_token_countor'].update(tokens)\n",
    "        \n",
    "        # 二元文法\n",
    "        g2_tokens = [''.join(tokens[i:i + 2]) for i in range(len(tokens[:-1]))]\n",
    "        stator['bi_gram_token_cnt'] += len(g2_tokens)\n",
    "        stator['bi_gram_token_countor'].update(g2_tokens)\n",
    "    return stator\n",
    "\n",
    "\n",
    "def stat_file(filename, stator, custom_reader=None, func_extract_sentence=lambda x: x, skip_lines=0, ignore_exp=False):\n",
    "    \"\"\"读取文件统计词频\"\"\"\n",
    "    with fileinput.input(filename, openhook=fileinput.hook_encoded(\"utf-8\")) as f:\n",
    "        reader = custom_reader(f) if custom_reader else f\n",
    "        for id,line in enumerate(reader):\n",
    "            if id % 5000 == 0:\n",
    "                print(str(id) + \" ...\")\n",
    "            if id < skip_lines:\n",
    "                continue\n",
    "            try:\n",
    "                stator = calc_frequency(stator, func_extract_sentence(line))\n",
    "            except Exception as e:\n",
    "                if not ignore_exp:\n",
    "                    raise e\n",
    "    return stator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计结果\n",
    "stator = {\n",
    "    # 一元文法总词数\n",
    "    \"uni_gram_token_cnt\": 0,\n",
    "    # 一元文法词频\n",
    "    \"uni_gram_token_countor\": Counter(),\n",
    "    # 二元文法总词数\n",
    "    \"bi_gram_token_cnt\": 0,\n",
    "    # 二元文法词频\n",
    "    \"bi_gram_token_countor\": Counter()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ...\n",
      "5000 ...\n",
      "10000 ...\n"
     ]
    }
   ],
   "source": [
    "# 读取 train 文件\n",
    "stator = stat_file(\"./res/train.txt\", stator, func_extract_sentence=lambda x: x.split(\"++$++\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76593, 62960)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stator['uni_gram_token_cnt'], stator['bi_gram_token_cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ...\n",
      "5000 ...\n",
      "10000 ...\n",
      "15000 ...\n",
      "20000 ...\n",
      "25000 ...\n",
      "30000 ...\n",
      "35000 ...\n",
      "40000 ...\n",
      "45000 ...\n",
      "50000 ...\n",
      "55000 ...\n",
      "60000 ...\n",
      "65000 ...\n",
      "70000 ...\n",
      "75000 ...\n",
      "80000 ...\n",
      "85000 ...\n",
      "90000 ...\n",
      "95000 ...\n",
      "100000 ...\n",
      "105000 ...\n",
      "110000 ...\n",
      "115000 ...\n",
      "120000 ...\n",
      "125000 ...\n",
      "130000 ...\n",
      "135000 ...\n",
      "140000 ...\n",
      "145000 ...\n",
      "150000 ...\n",
      "155000 ...\n",
      "160000 ...\n",
      "165000 ...\n",
      "170000 ...\n",
      "175000 ...\n",
      "180000 ...\n",
      "185000 ...\n",
      "190000 ...\n",
      "195000 ...\n",
      "200000 ...\n",
      "205000 ...\n",
      "210000 ...\n",
      "215000 ...\n",
      "220000 ...\n",
      "225000 ...\n",
      "230000 ...\n",
      "235000 ...\n",
      "240000 ...\n",
      "245000 ...\n",
      "250000 ...\n",
      "255000 ...\n",
      "260000 ...\n"
     ]
    }
   ],
   "source": [
    "# 读取 movie_comments.csv\n",
    "import csv\n",
    "stator = stat_file(\"./res/movie_comments.csv\", stator, custom_reader=csv.reader, func_extract_sentence=lambda x: x[3], skip_lines=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4643686, 3646353)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stator['uni_gram_token_cnt'], stator['bi_gram_token_cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def prob_uni_gram(word, stator): \n",
    "    \"\"\"计算一个词的概率\"\"\"\n",
    "    if word in stator['uni_gram_token_countor']:\n",
    "        return stator['uni_gram_token_countor'][word] / stator['uni_gram_token_cnt']\n",
    "    else:\n",
    "        return 1 / stator['uni_gram_token_cnt']\n",
    "\n",
    "prob_1 = partial(prob_uni_gram, stator=stator)\n",
    "\n",
    "\n",
    "def prob_bi_gram(word1, word2, stator):\n",
    "    \"\"\"计算两个词的概率\"\"\"\n",
    "    if word1 + word2 in stator['bi_gram_token_countor']:\n",
    "        return stator['bi_gram_token_countor'][word1+word2] / stator['uni_gram_token_countor'][word2]\n",
    "    else:\n",
    "        return 1 / stator['bi_gram_token_cnt']\n",
    "\n",
    "prob_2 = partial(prob_bi_gram, stator=stator)\n",
    "\n",
    "\n",
    "def get_probablity(sentence):\n",
    "    \"\"\"计算句子的概率\"\"\"\n",
    "    words = jieba.lcut(sentence)\n",
    "    \n",
    "    sentence_pro = 1\n",
    "    \n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_ = words[i+1]\n",
    "        \n",
    "        probability = prob_2(word, next_)\n",
    "        \n",
    "        sentence_pro *= probability\n",
    "    \n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.011205107322071303, 2.153461711235428e-07)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_1(\"我\"), prob_1(\"我啊啊\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.015494918970165614, 2.962699611886351e-05, 2.7424662395549746e-07)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_2(\"看\",\"电影\"), prob_2(\"打\",\"电影\"), prob_2(\"打\",\"球\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7468720703359976e-06"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity(\"我们去看电影吧\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(): # you code here\n",
    "    return sorted([(i, get_probablity(i)) for i in generate_n(hello_grammar_str, target=\"hello_stmt\", n=200)], key=lambda x: x[1], reverse=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('你们好,我们是周杰伦,我喜欢打水', 6.929720381496959e-38)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "1. 词库偏小，很多词没有在词库中出现过。\n",
    "2. 另外感觉规则不能自由度太大，比如爱好就可以直接是 打篮球/看电影，而不应该保留太大的自由度。\n",
    "3. 而且感觉二元文法只对一句话判断的概率比较好，如果是多句话，那么前后可能毫无关联，出现没有意义的句子。\n",
    "\n",
    "解决的话比较能快速提升效果就是改进规则，但是这就感觉更“有多少人工就有多少智能”了😂。 \n",
    "\n",
    "其他的暂时不知道怎么解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
